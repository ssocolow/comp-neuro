{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163fd0fa",
   "metadata": {},
   "source": [
    "## Assignment 07 (60 pts) - Due December 4th, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ca8b7",
   "metadata": {},
   "source": [
    "(5 pts) Be sure that your code is well-documented for reproducibility. Set a random seed at the top of your code for reproducibility (`np.random.seed(seed_number)`). Every function should have a docstring (simple functions can be short, longer functions should contain more detail). Before submitting, re-run your code. Figures should be labeled (axes labels, legends, etc), and clearly address any questions asked below! Remember, the whole goal is to have clear and concise communication of what you are finding computationally. \n",
    "\n",
    "Feel free to add cells as needed. You do not have to only use the cells that I provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35a7ba",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "***\n",
    "**Receptive field development with BCM rule**\n",
    "Consider 20 presynaptic neurons with firing rates $r_j$ that connect onto the same postsynaptic neuron which fires at a rate $r^{post}=\\sum_{j=1}^{20}w_jr_j$. Synaptic weights $w_j$ change according to the Bienenstock-Cooper-Munro (BCM) rule, with a hard lower bound $w_j\\ge 0$ and $ r_\\theta=10 $ Hz:\n",
    "\n",
    "$$\n",
    "    \\frac{d}{dt}w_j = \\eta r^{post}r_j(r^{post}-r_\\theta)\n",
    "$$\n",
    "\n",
    "The 20 inputs are organized in two groups of 10 inputs each. There are two possible input patterns, $P^k$, $k=1,2$. Choose learning rate of $\\eta=10^{-5}$. Initialize the weights as independently sampled from a uniform distribution between 0 and 1. Choose a time step size of $\\Delta t = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba8465",
   "metadata": {},
   "source": [
    "**(5 pts)** **(a)** The two possible input patterns are: $P^1 $ -- group 1 fires at 3 Hz and group 2 is quiescent; and $P^2 $ --  group 1 is quiescent and group 2 fires at 0.5 Hz.  For each time step, randomly select input pattern from $P^1$ and $P^2$ with 50\\% probability.   Set a upper bound of $w_j$ to be $w_{\\rm max} =10$.  Simulate $N_t =1000$ time steps.  How do weights $w_{j}$  evolve? Plot a heatmap of the weights using `imshow` with x-axis being time and y-axis being the presynaptic neuron ID, $j$. Plot the  firing rates of the  post-synaptic neuron, $r^{\\rm post} $, over time steps when the input patterns are $P^1$ or $P^2$, separately.   Show that the post-synaptic neuron becomes specialized to one group of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be26faf",
   "metadata": {},
   "source": [
    "**(5 pts)** **(b)** Similar to (a), except that the second pattern now is  $k =2 $ -- group 1 is quiescent and group 2 fires at 2 Hz. How do weights $w_{j}$  and  firing rate $r^{\\rm post} $ evolve? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1afd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6639f3",
   "metadata": {},
   "source": [
    "**(5 pts)** **(c)** As in (b), the two possible input patterns are: $P^1 $ -- group 1 fires at 3 Hz and group 2 is quiescent; and $P^2 $ --  group 1 is quiescent and group 2 fires at 2 Hz.  but make $r_\\theta$ a function of the time-average firing rate $r^{\\rm post} $ of the post-synaptic neuron. \n",
    "\\begin{equation}\n",
    "\\tau_{\\theta} \\frac{dr_\\theta}{dt} =  (r^{\\rm post})^2/10 -r_\\theta.  \n",
    "\\end{equation}\n",
    "with $\\tau_{\\theta}  =20 $.  Simulate $N_t =5000$ time steps. Plot a heatmap of the presynaptic weights over time. Plot the  firing rates of the  post-synaptic neuron, $r^{\\rm post}$, over time steps when the input patterns are $P^1$ or $P^2$, separately, and plot the threshold, $r_{\\theta}$ over time.     Repeat a few times with different initialization of weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3962107",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67024666",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "***\n",
    "**Perceptron binary classifier.**   Train a perceptron for binary classification. Each sample of input is  a $D$-dimensional vector,  $\\vec{x}=(x_1, x_2, \\ldots,  x_D)^T$. Each element $x_i$ is independent and normally distributed with standard deviation 1 and different mean.   Consider two categories of inputs.  Category 1 ($y=+1$): the mean of $\\vec{x}$ is $\\vec{c}_1$;  Category 2 ($y=-1$): the mean of $\\vec{x}$ is $\\vec{c}_2$ which is not equal to $\\vec{c}_1$. Generate $K_{\\rm train} = 10000$ training samples and  $K_{\\rm test} = 1000$ test samples  for each group.   \n",
    "\n",
    "The prediction of the perceptron classifier is   $\\hat{y} = {\\rm sign}(\\sum_{i=1}^D w_i x_i +b)$, where  $\\vec{w} = (w_1, w_2, \\ldots,  w_D)^T$  is the connection weight vector and $b$ is the bias.\n",
    "\n",
    "Initialize $\\vec{w}$ and $b$ to be zeros. Randomize the order of training samples.  For each training sample $\\vec{x}^{(k)}$, compute the output activity  $\\hat{y} = {\\rm sign}(\\vec{w} \\cdot \\vec{x}^{(k)}  +b)$. Then update  $\\vec{w}$ and $b$  using the perceptron learning rule: \n",
    "\n",
    "$$\n",
    "\\vec{w} \\rightarrow \\vec{w}+ \\eta (y^{(k)} - \\hat{y}) \\vec{x}^{(k)} \\\\\n",
    "b \\rightarrow b+\\eta (y^{(k)} - \\hat{y})\n",
    "$$\n",
    "\n",
    "where $\\eta = 0.001$ is the learning rate.  Iterate through all training samples, $k=1, 2, \\ldots, K_{\\rm train}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7802f7",
   "metadata": {},
   "source": [
    "**(5 pts)** **(a)** First choose $D=2$. Let $\\vec{c}_1=[2,2]^T$ and  $\\vec{c}_2=[-2, -2]^T$.  Plot the test samples $\\vec{x}$ from category 1 in blue and the test samples from category 2 in red on the $x_1-x_2$ plane. After training, plot the decision boundary of the trained  linear classifier,  which is the line of $w_1 x_1 +w_2 x_2 +b = 0$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initial Set up - Don't change this code ###\n",
    "K = 10000\n",
    "D = 2\n",
    "center1 = 2 * np.ones((D, 1))\n",
    "center2 = -2 * np.ones((D, 1))\n",
    "x1 = np.random.randn(D, K) + center1\n",
    "y1 = np.ones(K)\n",
    "x2 = np.random.randn(D, K) + center2\n",
    "y2 = -1 * np.ones(K)\n",
    "X = np.concatenate([x1, x2], axis=1)\n",
    "Y = np.concatenate([y1, y2])\n",
    "permutes = np.random.permutation(2 * K)\n",
    "X = X[:, permutes]\n",
    "Y = Y[permutes]\n",
    "\n",
    "Ktest = 1000\n",
    "x1_test = np.random.randn(D, Ktest) + center1\n",
    "y1_test = np.ones(Ktest)\n",
    "x2_test = np.random.randn(D, Ktest) + center2\n",
    "y2_test = -1 * np.ones(Ktest)\n",
    "Xtest = np.concatenate([x1_test, x2_test], axis=1)\n",
    "Ytest = np.concatenate([y1_test, y2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b571ecb",
   "metadata": {},
   "source": [
    "**(5 pts)** **(b)** Compute the accuracy of the classifier on test samples, which is the percentage of samples where  the prediction $\\hat{y}^{(k)}  = {\\rm sign}(\\vec{w} \\cdot \\vec{x}^{(k)} +b)$ equals the data category $y^{(k)}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641d788",
   "metadata": {},
   "source": [
    "**(5 pts)** **(c)** Compare the final result of  $\\vec{w}$ and the vector connecting the centers of inputs from the two categories,  $\\vec{m} = \\vec{c}_1 - \\vec{c}_2$.  Compute the overlap between $\\vec{w}$ and $\\vec{m}$ as $p = \\frac{\\sum_{i=1}^D w_i m_i }{\\sqrt{(\\sum_{i=1}^D w_i^2)(\\sum_{i=1}^D m_i^2)}}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0b9e6",
   "metadata": {},
   "source": [
    "\n",
    "**(5 pts)** **(d)** Vary $D$ from 1 to 20 and let $\\vec{c}_1=[0.5,0.5, \\ldots, 0.5]^T$ and  $\\vec{c}_2=[-0.5, -0.5,  \\ldots, -0.5]^T$.  For each $D$, do 10 repetitions of the training process. Make a scatter plot of accuracy  and a scatter plot of the overlap between $\\vec{w}$ and $\\vec{m}$ across $D$ for all repetitions.  Show that the accuracy and overlap between $\\vec{w}$ and $\\vec{m}$ increase with the dimension of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b290891",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7ac06",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a192c2f",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "***\n",
    "**PCA on MNIST dataset.** The MNIST dataset is a large database of handwritten digits that is often used to benchmark machine learning algorithms. Each image is a 28x28 pixel grayscale imiage. For convenience, each 28x28 pixel image is unravelled into a single 784 (28^2) element vector. In the data file on Glow, `mnist.zip`, there is a training data set, trainX of 60,000 images and a testing data set testX of 10,000 images. Each row represents a different image, and each column represents a different pixel. \n",
    "\n",
    "You can visualize an image $k$ in the training set with `ax.imshow(trainX[k,:].reshape(28,28))`, an example is below. Each image has an associated label denoting which digit the image represents (0-9). The labels for the training images are saved in trainY, and the labes for the test images are saved in testY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in MNIST data ###\n",
    "### Don't chage this code ###\n",
    "### Assumes MNIST data in same directory as notebook ###\n",
    "trainX = np.loadtxt(\"trainX.txt\")\n",
    "testX = np.loadtxt(\"testX.txt\")\n",
    "trainY = np.loadtxt(\"trainY.txt\")\n",
    "testY = np.loadtxt(\"testY.txt\")\n",
    "X = np.concatenate([trainX, testX])\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Example of plotting one image\n",
    "plt.figure()\n",
    "plt.imshow(X[378, :].reshape(28, 28), aspect=\"auto\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc418ae6",
   "metadata": {},
   "source": [
    "**(5 pts)** **(a)**  Compute the mean of each column of $X$, $Xmean = mean(X)$ and subtract the mean from each column. Compute the covariance of $X$ using `np.cov` with the optional argument `rowvar=False` and eigenvalues of $\\lambda_i$ of the covariance matrix. Determine the eigenvalues and eigenvectors with `np.linalg.eigh`. Plot all the eigenvalues sorted large to small (don't forget to rearrange the eigenvectors for later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74192b05",
   "metadata": {},
   "source": [
    "**(5 pts)** **(b)** The intrinsic dimensionality of the dataset can be studied via the fraction of variance explained by the top $d$ eigenvalues:\n",
    "\n",
    "$$\n",
    "\\text{VarExplained}(d) = \\frac{\\sum_{i=1}^d \\lambda_i}{\\sum_{i=1}^N\\lambda_i} \\quad N = 784\n",
    "$$\n",
    "\n",
    "The intrinsic dimensionality is often quantified by the $d$ necessary to explain a large proportion of the total variance of the data (often a defined threshold, e.g. 90%). Plot VarExplained $(d)$ for $d=1$ to $d=784$. What is the minimum $d$ such that the threshold of 90% is reached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae71f9b",
   "metadata": {},
   "source": [
    "**(5 pts)** **(c)** Let's reconstruct the data using only a subset of the principal components. Try with $D = 20$, $D= 80$, and $D = 200$. \n",
    "\n",
    "For each $D$, reconstruct $X$ with only $D$ dimensions. In other words, project onto the top $D$ principal compoents, $Y_D = X_0 V_D$ where $X_0$ is $X-mean(X)$ (from part (a)) and $V_D$ is the matrix of eigenvectors with $D$ dimensions. Then, find the reconstrcuted $X_{recon}$ by $X_{recon} = Y_D V_D^T$. Don't forget to add back the mean(X)!\n",
    "\n",
    "Choose 5 example images and plot both the original and reconstrcuted images for all values of $D$. This will result in a 20 panel figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff725bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf80f39",
   "metadata": {},
   "source": [
    "**(5 pts)** **(d)** Add Gaussian distributed independent noise to each pixel of the images $X$. Set the standard deviaion of the noise to be $0.2*max(X)$ and the mean of the noise to be 0. Repeat (a)-(c) from above with the noisy data. For part (a), you can skip the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3110db",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bdd596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
